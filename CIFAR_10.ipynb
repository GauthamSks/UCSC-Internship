{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ithG-X2YL2BO",
        "outputId": "2f5e0bf7-6efc-44a0-fbb0-6a1cc1e0580a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG3NPC0kUiC1"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NartgRR6ZAWZ"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = K.datasets.cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00ZQDUNGZgkw",
        "outputId": "7deeac18-623d-4f22-ab6c-2f58d737d436"
      },
      "source": [
        "VGG_16 = applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Creating dictionary that maps layer names to the layers\n",
        "layer_dict = dict([(layer.name, layer) for layer in VGG_16.layers])\n",
        "\n",
        "OutPut = layer_dict['block5_pool'].output \n",
        "OutPut = GlobalAveragePooling2D()(OutPut)\n",
        "\n",
        "VGG_Base = Model(inputs=VGG_16.input,outputs=OutPut)\n",
        "\n",
        "# Make sure that the pre-trained bottom layers are not trainable\n",
        "for layer in VGG_Base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "VGG_Base.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSpdot4foKIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84867d5b-fe91-4a17-84e6-03668f094be7"
      },
      "source": [
        "from tensorflow.keras.layers import UpSampling2D\n",
        "\n",
        "VGG_Feature_Matrix = np.zeros((x_train.shape[0],512))\n",
        "count = 0\n",
        "\n",
        "for idx,img in enumerate(x_train):\n",
        "  count+=1\n",
        "  if(count%1000 == 0):\n",
        "    print(count,\" Images Done\")\n",
        "  img = tf.expand_dims(img, axis=0)\n",
        "  up_sampled_image = UpSampling2D(size=(7, 7))(img)\n",
        "  \n",
        "  feature = VGG_Base.predict(up_sampled_image)\n",
        "  feature = np.squeeze(feature)\n",
        "\n",
        "  VGG_Feature_Matrix[idx] = feature"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000  Images Done\n",
            "2000  Images Done\n",
            "3000  Images Done\n",
            "4000  Images Done\n",
            "5000  Images Done\n",
            "6000  Images Done\n",
            "7000  Images Done\n",
            "8000  Images Done\n",
            "9000  Images Done\n",
            "10000  Images Done\n",
            "11000  Images Done\n",
            "12000  Images Done\n",
            "13000  Images Done\n",
            "14000  Images Done\n",
            "15000  Images Done\n",
            "16000  Images Done\n",
            "17000  Images Done\n",
            "18000  Images Done\n",
            "19000  Images Done\n",
            "20000  Images Done\n",
            "21000  Images Done\n",
            "22000  Images Done\n",
            "23000  Images Done\n",
            "24000  Images Done\n",
            "25000  Images Done\n",
            "26000  Images Done\n",
            "27000  Images Done\n",
            "28000  Images Done\n",
            "29000  Images Done\n",
            "30000  Images Done\n",
            "31000  Images Done\n",
            "32000  Images Done\n",
            "33000  Images Done\n",
            "34000  Images Done\n",
            "35000  Images Done\n",
            "36000  Images Done\n",
            "37000  Images Done\n",
            "38000  Images Done\n",
            "39000  Images Done\n",
            "40000  Images Done\n",
            "41000  Images Done\n",
            "42000  Images Done\n",
            "43000  Images Done\n",
            "44000  Images Done\n",
            "45000  Images Done\n",
            "46000  Images Done\n",
            "47000  Images Done\n",
            "48000  Images Done\n",
            "49000  Images Done\n",
            "50000  Images Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDCMwETkXhDy"
      },
      "source": [
        "# np.save(\"/content/drive/MyDrive/Models/CIFAR_10_VGG_Feature_Matrix.npy\",VGG_Feature_Matrix)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUmRoV7jj8Uv"
      },
      "source": [
        "VGG_Feature_Matrix = np.load(\"/content/drive/MyDrive/Models/CIFAR_10_VGG_Feature_Matrix.npy\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRpvnWqolRYK"
      },
      "source": [
        "# Larger Subsets\n",
        "First_25k_Images = VGG_Feature_Matrix[0:25000,:]\n",
        "# Second_25_Images = VGG_Feature_Matrix[25000:,:]\n",
        "\n",
        "# Smaller Subsets for Experimentation\n",
        "# First_10k_Images = VGG_Feature_Matrix[0:10000,:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLCUcXbvsMr0"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def cosDistance(features):\n",
        "    # features: N*M matrix. N features, each features is M-dimension.\n",
        "    features = F.normalize(features, dim=1) # each feature's l2-norm should be 1 \n",
        "    similarity_matrix = torch.matmul(features, features.T)\n",
        "    distance_matrix = 1.0 - similarity_matrix\n",
        "    return distance_matrix"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq9FS6ltsTOy"
      },
      "source": [
        "cos_sim_mat = cosDistance(torch.from_numpy(First_25k_Images))\n",
        "# 0.0825\n",
        "threshold = 0.5\n",
        "adj_matrix = torch.lt(cos_sim_mat, threshold).int()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha4tJpKqmX5G",
        "outputId": "bfd34805-480c-4f13-8559-40eccb51cd30"
      },
      "source": [
        "unique, counts = np.unique(y_train[0:25000], return_counts=True)\n",
        "class_to_counts = dict(zip(unique, counts))\n",
        "\n",
        "print(class_to_counts)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 2491, 1: 2518, 2: 2515, 3: 2522, 4: 2490, 5: 2411, 6: 2537, 7: 2530, 8: 2507, 9: 2479}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1JiufWPp6As"
      },
      "source": [
        "idx_to_class = {}\n",
        "\n",
        "for idx,cls in enumerate(y_train[0:10000].tolist()):\n",
        "  idx_to_class[idx] = cls[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y53cNtGrgs4"
      },
      "source": [
        "class_to_idx = {}\n",
        "\n",
        "for idx,cls in enumerate(y_train[0:10000].tolist()):\n",
        "  if(cls[0] not in class_to_idx.keys()):\n",
        "    class_to_idx[cls[0]] = [idx]\n",
        "  else:\n",
        "    class_to_idx[cls[0]].append(idx)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA9IFuyStZEU"
      },
      "source": [
        "result_dict = {}\n",
        "for cls in class_to_idx.keys():\n",
        "  class_adj_sum = 0\n",
        "  for idx in class_to_idx[cls]:\n",
        "    row_vals_sum = torch.sum(adj_matrix[idx])\n",
        "    class_adj_sum += row_vals_sum\n",
        "  generalization_factor = ((class_to_counts[cls]*class_to_counts[cls]) - class_adj_sum)/(class_to_counts[cls]*class_to_counts[cls])\n",
        "  result_dict[cls] = generalization_factor"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9nXMUmCtafs",
        "outputId": "1bc350c3-7c06-4d98-e082-9c99ded1dca5"
      },
      "source": [
        "print(\"Threshold:\",threshold,\"\\n\")\n",
        "result_dict"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.5 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: tensor(-2.6240),\n",
              " 1: tensor(-2.7125),\n",
              " 2: tensor(-2.8265),\n",
              " 3: tensor(-2.8620),\n",
              " 4: tensor(-2.8435),\n",
              " 5: tensor(-2.9220),\n",
              " 6: tensor(-2.7394),\n",
              " 7: tensor(-2.8426),\n",
              " 8: tensor(-2.8606),\n",
              " 9: tensor(-2.9009)}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}